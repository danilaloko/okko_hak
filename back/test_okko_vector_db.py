#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–µ—Ä–≤—ã—Ö 10 –∑–∞–ø–∏—Å–µ–π Okko
"""

import pandas as pd
import numpy as np
import json
import re
import os
from typing import List, Dict, Any, Tuple
from datetime import datetime
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_okko_data_test(parquet_path: str = "data/catalog_okko.parquet", limit: int = 5000) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–µ—Ä–≤—ã–µ N –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ Okko –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–≤—ã–µ {limit} –∑–∞–ø–∏—Å–µ–π –∏–∑ {parquet_path}")
    
    try:
        df = pd.read_parquet(parquet_path)
        df_test = df.head(limit).copy()
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df_test)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        return df_test
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def clean_text(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç"""
    if pd.isna(text) or text == "":
        return ""
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
    text = re.sub(r'\s+', ' ', str(text).strip())
    
    # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
    text = re.sub(r'<[^>]+>', '', text)
    
    return text

def create_enhanced_text(row: pd.Series) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    parts = []
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä–æ–∫–∏
    def safe_str(value):
        if isinstance(value, (list, tuple, np.ndarray)):
            if len(value) == 0:
                return ""
            return str(value)
        if pd.isna(value):
            return ""
        return str(value)
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    serial_name = safe_str(row["serial_name"])
    if serial_name and serial_name.strip():
        parts.append(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {clean_text(serial_name)}")
    
    # –û–ø–∏—Å–∞–Ω–∏–µ (—Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ –¥–ª—è RAG)
    description = safe_str(row["description"])
    if description and description.strip():
        desc = clean_text(description)
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        if len(desc) > 500:
            desc = desc[:500] + "..."
        parts.append(f"–û–ø–∏—Å–∞–Ω–∏–µ: {desc}")
    
    # –ñ–∞–Ω—Ä—ã —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    genres = safe_str(row["genres"])
    if genres and genres.strip():
        genres_clean = clean_text(genres)
        parts.append(f"–ñ–∞–Ω—Ä—ã: {genres_clean}")
    
    # –ê–∫—Ç–µ—Ä—ã (–ø–µ—Ä–≤—ã–µ 5 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞)
    actors = safe_str(row["actors"])
    if actors and actors.strip():
        actors_clean = clean_text(actors)
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã—Ö 5 –∞–∫—Ç–µ—Ä–æ–≤
        actors_list = [actor.strip() for actor in actors_clean.split(",") if actor.strip()][:5]
        if actors_list:
            parts.append(f"–í —Ä–æ–ª—è—Ö: {', '.join(actors_list)}")
    
    # –†–µ–∂–∏—Å—Å–µ—Ä
    director = safe_str(row["director"])
    if director and director.strip():
        director_clean = clean_text(director)
        parts.append(f"–†–µ–∂–∏—Å—Å–µ—Ä: {director_clean}")
    
    # –°—Ç—Ä–∞–Ω–∞ –∏ —Å—Ç—É–¥–∏—è
    country = safe_str(row["country"])
    if country and country.strip():
        country_clean = clean_text(country)
        parts.append(f"–°—Ç—Ä–∞–Ω–∞: {country_clean}")
    
    studio = safe_str(row["studio_name"])
    if studio and studio.strip():
        studio_clean = clean_text(studio)
        parts.append(f"–°—Ç—É–¥–∏—è: {studio_clean}")
    
    # –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_type = safe_str(row["content_type"])
    if content_type and content_type.strip():
        content_type_clean = clean_text(content_type)
        parts.append(f"–¢–∏–ø: {content_type_clean}")
    
    # –í–æ–∑—Ä–∞—Å—Ç–Ω–æ–π —Ä–µ–π—Ç–∏–Ω–≥
    if pd.notna(row["age_rating"]) and row["age_rating"] > 0:
        parts.append(f"–í–æ–∑—Ä–∞—Å—Ç–Ω–æ–π —Ä–µ–π—Ç–∏–Ω–≥: {row['age_rating']}+")
    
    return " | ".join(parts)

def create_metadata(row: pd.Series, index: int) -> Dict[str, Any]:
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä–æ–∫–∏
    def safe_str(value):
        if isinstance(value, (list, tuple, np.ndarray)):
            if len(value) == 0:
                return ""
            return str(value)
        if pd.isna(value):
            return ""
        return str(value)
    
    metadata = {
        "id": index,
        "title": clean_text(safe_str(row["serial_name"])),
        "content_type": clean_text(safe_str(row["content_type"])),
        "country": clean_text(safe_str(row["country"])),
        "age_rating": float(row["age_rating"]) if pd.notna(row["age_rating"]) else None,
        "url": safe_str(row["url"]),
        "studio": clean_text(safe_str(row["studio_name"])),
        "director": clean_text(safe_str(row["director"])),
        "release_date": safe_str(row["release_date"]) if pd.notna(row["release_date"]) else None
    }
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∂–∞–Ω—Ä–æ–≤
    genres_str = safe_str(row["genres"])
    if genres_str and genres_str.strip():
        genres_text = clean_text(genres_str)
        metadata["genres"] = [genre.strip() for genre in genres_text.split(",") if genre.strip()]
    else:
        metadata["genres"] = []
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–∫—Ç–µ—Ä–æ–≤
    actors_str = safe_str(row["actors"])
    if actors_str and actors_str.strip():
        actors_text = clean_text(actors_str)
        metadata["actors"] = [actor.strip() for actor in actors_text.split(",") if actor.strip()]
    else:
        metadata["actors"] = []
    
    return metadata

def create_embeddings(df: pd.DataFrame, model_kind: str, model) -> np.ndarray:
    """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ–∏–ª—å–º–æ–≤"""
    logger.info("–°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
    
    texts = []
    for idx, row in df.iterrows():
        text = create_enhanced_text(row)
        texts.append(text)
        logger.info(f"–¢–µ–∫—Å—Ç {idx}: {text[:100]}...")
    
    logger.info(f"–°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {len(texts)} —Ñ–∏–ª—å–º–æ–≤...")
    
    if model_kind == "st":
        embeddings = model.encode(texts, normalize_embeddings=True, show_progress_bar=True)
    else:
        embeddings = model.fit_transform(texts).astype(np.float32)
        A = embeddings.toarray()
        norms = np.linalg.norm(A, axis=1, keepdims=True) + 1e-9
        embeddings = A / norms
    
    logger.info(f"–°–æ–∑–¥–∞–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {embeddings.shape}")
    return embeddings

def analyze_okko_data(df: pd.DataFrame) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ Okko –∏ —Å–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
    logger.info("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ Okko...")
    
    # –ê–Ω–∞–ª–∏–∑ –∂–∞–Ω—Ä–æ–≤
    all_genres = []
    for genres_str in df["genres"].dropna():
        if isinstance(genres_str, str):
            genres = [g.strip() for g in genres_str.split(",") if g.strip()]
            all_genres.extend(genres)
    
    genre_counts = pd.Series(all_genres).value_counts().to_dict()
    unique_genres = list(genre_counts.keys())
    
    # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_types = df["content_type"].astype(str).value_counts().to_dict()
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω
    countries = df["country"].astype(str).value_counts().to_dict()
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Ç—É–¥–∏–π
    studios = df["studio_name"].astype(str).value_counts().to_dict()
    
    # –ê–Ω–∞–ª–∏–∑ –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
    age_ratings = df["age_rating"].dropna()
    age_rating_stats = {
        "min": float(age_ratings.min()) if len(age_ratings) > 0 else None,
        "max": float(age_ratings.max()) if len(age_ratings) > 0 else None,
        "mean": float(age_ratings.mean()) if len(age_ratings) > 0 else None
    }
    
    # –ê–Ω–∞–ª–∏–∑ –∞–∫—Ç–µ—Ä–æ–≤
    all_actors = []
    for actors_str in df["actors"].dropna():
        if isinstance(actors_str, str):
            actors = [a.strip() for a in actors_str.split(",") if a.strip()]
            all_actors.extend(actors)
    
    actor_counts = pd.Series(all_actors).value_counts().to_dict()
    top_actors = list(actor_counts.keys())[:50]  # –¢–æ–ø 50 –∞–∫—Ç–µ—Ä–æ–≤
    
    metadata = {
        "total_movies": len(df),
        "genres": unique_genres,
        "genre_counts": genre_counts,
        "content_types": content_types,
        "countries": countries,
        "studios": studios,
        "age_rating_stats": age_rating_stats,
        "top_actors": top_actors,
        "actor_counts": {k: v for k, v in list(actor_counts.items())[:100]},
        "created_at": datetime.now().isoformat(),
        "data_source": "okko_catalog_test"
    }
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(unique_genres)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∂–∞–Ω—Ä–æ–≤")
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(content_types)} —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(countries)} —Å—Ç—Ä–∞–Ω")
    
    return metadata

def save_vector_db(df: pd.DataFrame, embeddings: np.ndarray, metadata: Dict[str, Any], 
                  output_dir: str = "data") -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    os.makedirs(output_dir, exist_ok=True)
    
    logger.info("–°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
    
    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏
    records_metadata = []
    for idx, row in df.iterrows():
        record_meta = create_metadata(row, idx)
        records_metadata.append(record_meta)
        logger.info(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ {idx}: {record_meta['title']}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
    df.to_pickle(f"{output_dir}/okko_test_movies_df.pkl")
    np.save(f"{output_dir}/okko_test_embeddings.npy", embeddings)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    with open(f"{output_dir}/okko_test_metadata.json", "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–µ–π
    with open(f"{output_dir}/okko_test_records_metadata.json", "w", encoding="utf-8") as f:
        json.dump(records_metadata, f, ensure_ascii=False, indent=2)
    
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {output_dir}")
    logger.info(f"  - okko_test_movies_df.pkl: {len(df)} —Ñ–∏–ª—å–º–æ–≤")
    logger.info(f"  - okko_test_embeddings.npy: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ {embeddings.shape}")
    logger.info(f"  - okko_test_metadata.json: –æ–±—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ")
    logger.info(f"  - okko_test_records_metadata.json: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–µ–π")

def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
    
    try:
        from sentence_transformers import SentenceTransformer
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        model_kind = "st"
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º SentenceTransformer (multilingual)")
        return model, model_kind
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å SentenceTransformer: {e}")
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            model = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))
            model_kind = "tfidf"
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF")
            return model, model_kind
        except Exception as e2:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å: {e2}")
            raise

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("=== –¢–ï–°–¢–û–í–û–ï –°–û–ó–î–ê–ù–ò–ï –í–ï–ö–¢–û–†–ù–û–ô –ë–î –ò–ó OKKO –î–ê–ù–ù–´–• ===")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π)
        df = load_okko_data_test(limit=1000)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –∑–∞–ø–∏—Å–∏
        logger.info("–ü–µ—Ä–≤—ã–µ –∑–∞–ø–∏—Å–∏:")
        for idx, row in df.iterrows():
            logger.info(f"  {idx}: {row['serial_name']}")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        metadata = analyze_okko_data(df)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        model, model_kind = load_model()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings = create_embeddings(df, model_kind, model)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        save_vector_db(df, embeddings, metadata)
        
        logger.info("=== –¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù ===")
        logger.info("–¢–µ—Å—Ç–æ–≤–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö Okko —Å–æ–∑–¥–∞–Ω–∞!")
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–ï–°–¢–ê:")
        print(f"   ‚Ä¢ –¢–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∏–ª—å–º–æ–≤: {len(df)}")
        print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embeddings.shape}")
        print(f"   ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∂–∞–Ω—Ä–æ–≤: {len(metadata['genres'])}")
        print(f"   ‚Ä¢ –¢–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {len(metadata['content_types'])}")
        print(f"   ‚Ä¢ –°—Ç—Ä–∞–Ω: {len(metadata['countries'])}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î: {e}")
        raise

if __name__ == "__main__":
    main()
