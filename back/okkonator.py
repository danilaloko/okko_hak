import os
import json
import numpy as np
import pandas as pd
import streamlit as st

# ====== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ======
ETA = 0.3                 # —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è
QUESTIONS_MAX = 10        # –º–∞–∫—Å–∏–º—É–º –≤–æ–ø—Ä–æ—Å–æ–≤ (–∫–æ—Ä–æ—Ç–∫–∏–π —Ä–µ–∂–∏–º)
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"  # 384-d

# ====== –î–∞–Ω–Ω—ã–µ ======
@st.cache_data
def load_movies():
    df = pd.read_csv("movies.csv")
    df["genres_list"] = df["genres"].fillna("").apply(lambda x: [g.strip() for g in x.split(";") if g.strip()])
    return df

movies = load_movies()

# ====== –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ======
@st.cache_resource
def load_model():
    try:
        from sentence_transformers import SentenceTransformer
        return SentenceTransformer(MODEL_NAME), "st"
    except Exception as e:
        st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å sentence-transformers, –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ TF-IDF (–∫–∞—á–µ—Å—Ç–≤–æ –Ω–∏–∂–µ).")
        from sklearn.feature_extraction.text import TfidfVectorizer
        vec = TfidfVectorizer(max_features=5000)
        return vec, "tfidf"

model, model_kind = load_model()

@st.cache_resource
def embed_items(df):
    texts = (df["title"].fillna("") + " " +
             df["overview"].fillna("") + " " +
             df["genres"].fillna(""))
    if model_kind == "st":
        X = model.encode(texts.tolist(), normalize_embeddings=True)
    else:
        X = model.fit_transform(texts.tolist()).astype(np.float32)
        # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        norms = np.linalg.norm(X.toarray(), axis=1, keepdims=True) + 1e-9
        X = X.toarray() / norms
    return X

ITEM_EMB = embed_items(movies)

# ====== –û—Å–∏ –ø—Ä–æ—Ñ–∏–ª—è (10 —à—Ç.) ======
AXES = [
    "tempo_slow",         # –º–µ–¥–ª–µ–Ω–Ω—ã–π/–∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω—ã–π
    "darkness",           # –º—Ä–∞—á–Ω–æ—Å—Ç—å/–Ω—É–∞—Ä
    "humor",              # —é–º–æ—Ä
    "novelty",            # –Ω–æ–≤–∏–∑–Ω–∞ (–Ω–æ–≤–∏–Ω–∫–∏ vs –∫–ª–∞—Å—Å–∏–∫–∞)
    "length_short",       # –∫–æ—Ä–æ—Ç–∫–æ–µ (<=110 –º–∏–Ω)
    "violence_tol",       # —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ –∂–µ—Å—Ç–∫–∏–º —Å—Ü–µ–Ω–∞–º
    "genre_crime",
    "genre_comedy",
    "genre_scifi",
    "genre_drama",
]

# ====== –í–æ–ø—Ä–æ—Å—ã: –∫–∞–∂–¥—ã–π –±—å–µ—Ç –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –æ—Å—è–º ======
# –û—Ç–≤–µ—Ç: -2, -1, 0, +1, +2
QUESTIONS = [
    {"id":"q1", "text":"–°–µ–π—á–∞—Å —Ö–æ—á–µ—Ç—Å—è —á–µ–≥–æ-—Ç–æ –ª—ë–≥–∫–æ–≥–æ –∏ —Ç—ë–ø–ª–æ–≥–æ?",
     "targets":{"darkness":-1, "humor":+1}},
    {"id":"q2", "text":"–û–∫–µ–π –º–µ–¥–ª–µ–Ω–Ω—ã–π, –∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω—ã–π —Ç–µ–º–ø?",
     "targets":{"tempo_slow":+1}},
    {"id":"q3", "text":"–Æ–º–æ—Ä –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω —Å–µ–≥–æ–¥–Ω—è?",
     "targets":{"humor":+1}},
    {"id":"q4", "text":"–ú—Ä–∞—á–Ω—ã–µ/—Ç—è–∂—ë–ª—ã–µ —Ç–µ–º—ã ‚Äî –Ω–æ—Ä–º–∞–ª—å–Ω–æ?",
     "targets":{"darkness":+1}},
    {"id":"q5", "text":"–•–æ—á–µ—Ç—Å—è –∫–æ—Ä–æ—Ç–∫–æ–≥–æ (–¥–æ ~110 –º–∏–Ω—É—Ç)?",
     "targets":{"length_short":+1}},
    {"id":"q6", "text":"–ì–æ—Ç–æ–≤—ã –∫ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É/—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–º—É?",
     "targets":{"novelty":+1}},
    {"id":"q7", "text":"–û–∫–µ–π —Å –∂—ë—Å—Ç–∫–∏–º–∏ —Å—Ü–µ–Ω–∞–º–∏ (–Ω–∞—Å–∏–ª–∏–µ)?",
     "targets":{"violence_tol":+1}},
    {"id":"q8", "text":"–¢—è–Ω–µ—Ç –±–æ–ª—å—à–µ –∫ –∫—Ä–∏–º–∏–Ω–∞–ª—É/–¥–µ—Ç–µ–∫—Ç–∏–≤—É, —á–µ–º –∫ —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–µ?",
     "targets":{"genre_crime":+1, "genre_scifi":-1}},
    {"id":"q9", "text":"–ö–æ–º–µ–¥–∏—è —Ç–æ–∂–µ –ø–æ–¥–æ–π–¥—ë—Ç?",
     "targets":{"genre_comedy":+1}},
    {"id":"q10","text":"–ì–æ—Ç–æ–≤—ã –∫ —Å–µ—Ä—å—ë–∑–Ω–æ–π –¥—Ä–∞–º–µ?",
     "targets":{"genre_drama":+1}},
]

LIKERT_LABELS = ["–Ω–µ—Ç (-2)","—Å–∫–æ—Ä–µ–µ –Ω–µ—Ç (-1)","–Ω–µ –∑–Ω–∞—é (0)","—Å–∫–æ—Ä–µ–µ –¥–∞ (+1)","–¥–∞ (+2)"]
LIKERT_VALUES = [-2,-1,0,1,2]

# ====== –í—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ ======
def init_state():
    if "theta" not in st.session_state:
        st.session_state.theta = {ax: 0.0 for ax in AXES}
    if "asked" not in st.session_state:
        st.session_state.asked = set()
    if "answers" not in st.session_state:
        st.session_state.answers = {}

def update_theta(answer_value, targets):
    # answer_value ‚àà {-2..+2} -> scale to [-1..+1]
    s = answer_value / 2.0
    for ax, weight in targets.items():
        # weight ‚àà {-1..+1}: –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Å–∏
        st.session_state.theta[ax] = float(np.clip(
            st.session_state.theta[ax] + ETA * weight * s, -1.0, 1.0
        ))

def profile_keywords(theta):
    # –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è "–ø—Ä–æ—Ñ–∏–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞" –∏–∑ –æ—Å–µ–π
    tokens = []
    if theta["tempo_slow"] > 0.2: tokens += ["slow-burn","atmospheric","character-driven"]
    if theta["darkness"] > 0.2: tokens += ["dark","gritty","noir"]
    if theta["darkness"] < -0.2: tokens += ["warm","feel-good","cozy"]
    if theta["humor"] > 0.2: tokens += ["funny","comedy","lighthearted"]
    if theta["novelty"] > 0.2: tokens += ["unconventional","experimental","fresh"]
    if theta["genre_crime"] > 0.2: tokens += ["crime","detective","mystery"]
    if theta["genre_comedy"] > 0.2: tokens += ["comedy"]
    if theta["genre_scifi"] > 0.2: tokens += ["sci-fi","science fiction","futuristic"]
    if theta["genre_drama"] > 0.2: tokens += ["drama","emotional"]
    # –¥–ª–∏–Ω–∞/–Ω–∞—Å–∏–ª–∏–µ ‚Äî –∫–∞–∫ —Ñ–∏–ª—å—Ç—Ä—ã, –Ω–µ –≤ —Ç–µ–∫—Å—Ç
    if not tokens:
        tokens = ["balanced","popular","well-rated"]
    return " ".join(tokens)

def embed_text(text):
    if model_kind == "st":
        emb = model.encode([text], normalize_embeddings=True)[0]
    else:
        X = model.transform([text]).astype(np.float32)
        v = X.toarray()[0]
        emb = v / (np.linalg.norm(v)+1e-9)
    return emb

def cosine_sim(a, B):
    # a: (d,), B: (n,d)
    return (B @ a) / (np.linalg.norm(a)+1e-9)

def filter_and_rank(theta, top_k=10):
    user_text = profile_keywords(theta)
    user_emb = embed_text(user_text)
    sims = cosine_sim(user_emb, ITEM_EMB)

    df = movies.copy()
    df["sim"] = sims

    # –ñ—ë—Å—Ç–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã
    if theta["violence_tol"] < 0:    # –ø—Ä–æ—Ç–∏–≤ –∂—ë—Å—Ç–∫–∏—Ö —Å—Ü–µ–Ω
        df = df[df["violence"] == 0]
    if theta["length_short"] > 0.2:  # –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º <=110
        df["length_penalty"] = np.where(df["runtime"] <= 110, 0.0, -0.1)
    else:
        df["length_penalty"] = 0.0

    # –ú—è–≥–∫–∏–π –±–æ–Ω—É—Å –∑–∞ –Ω–æ–≤–∏–∑–Ω—É/–∫–ª–∞—Å—Å–∏–∫—É
    year_bonus = 0.0
    if theta["novelty"] > 0.2:
        year_bonus = 0.05 * ((df["year"] - df["year"].min()) / (df["year"].max()-df["year"].min()+1e-9))
    elif theta["novelty"] < -0.2:
        year_bonus = -0.05 * ((df["year"] - df["year"].min()) / (df["year"].max()-df["year"].min()+1e-9))
    df["score"] = df["sim"] + df["length_penalty"] + year_bonus

    recs = df.sort_values("score", ascending=False).head(top_k)
    return recs[["id","title","genres","runtime","year","score"]]

def explain_card(row, theta):
    bits = []
    if theta["tempo_slow"] > 0.2: bits.append("–º–µ–¥–ª–µ–Ω–Ω—ã–π/–∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω—ã–π —Ç–µ–º–ø")
    if theta["darkness"] > 0.2: bits.append("–º—Ä–∞—á–Ω—ã–π –≤–∞–π–±")
    if theta["darkness"] < -0.2: bits.append("—Ç—ë–ø–ª—ã–π/–ª–∞–π—Ç–æ–≤—ã–π –≤–∞–π–±")
    if theta["humor"] > 0.2 and "Comedy" in row["genres"]: bits.append("—Ö–æ—Ç–µ–ª–∏ —é–º–æ—Ä ‚Äî —ç—Ç–æ –∫–æ–º–µ–¥–∏—è")
    if theta["length_short"] > 0.2 and row["runtime"] <= 110: bits.append("–∫–æ—Ä–æ—Ç–∫–∏–π (‚â§110 –º–∏–Ω)")
    if theta["genre_crime"] > 0.2 and "Crime" in row["genres"]: bits.append("–∫—Ä–∏–º–∏–Ω–∞–ª/–¥–µ—Ç–µ–∫—Ç–∏–≤")
    if theta["genre_scifi"] > 0.2 and "Sci-Fi" in row["genres"]: bits.append("sci-fi")
    if theta["genre_drama"] > 0.2 and "Drama" in row["genres"]: bits.append("–¥—Ä–∞–º–∞")
    if theta["novelty"] > 0.2 and row["year"] >= 2015: bits.append("—Å–≤–µ–∂–µ–µ")
    return " ¬∑ ".join(bits[:3]) if bits else "—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –æ–±—â–µ–º—É –≤–∫—É—Å—É"

def next_question():
    # –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π –Ω–µ –∑–∞–¥–∞–Ω–Ω—ã–π; —á—É—Ç—å-—á—É—Ç—å —É–º–Ω–µ–µ ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–µ–º,
    # –≥–¥–µ theta –±–ª–∏–∂–µ –∫ 0 –∏ –∫–æ—Ç–æ—Ä—ã—Ö –µ—â—ë –Ω–µ —Ç—Ä–æ–≥–∞–ª–∏.
    remaining = [q for q in QUESTIONS if q["id"] not in st.session_state.asked]
    if not remaining:
        return None
    # –ø—Ä–∏–º–∏—Ç–∏–≤–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–∫–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö –æ—Å–µ–π –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å
    def prio(q):
        unseen_axes = sum(1 for ax in q["targets"].keys())
        flatness = sum(1.0 - abs(st.session_state.theta.get(ax, 0.0)) for ax in q["targets"].keys())
        return (unseen_axes, flatness)
    remaining.sort(key=prio, reverse=True)
    return remaining[0]

# ====== UI ======
st.set_page_config(page_title="–û–∫–∫–æ–Ω–∞—Ç–æ—Ä (–º–∏–Ω–∏–º–∞–ª)", page_icon="üé¨", layout="centered")
st.title("üé¨ –û–∫–∫–æ–Ω–∞—Ç–æ—Ä ‚Äî –±—ã—Å—Ç—Ä–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∫—É—Å–∞")

init_state()

col1, col2 = st.columns([3,2])
with col1:
    st.markdown("–û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å–æ–≤ ‚Äî –ø–æ–ª—É—á–∏–º –±–∞–∑–æ–≤—ã–π –ø—Ä–æ—Ñ–∏–ª—å –∏ –ø–æ–∫–∞–∂–µ–º –ø–æ–¥–±–æ—Ä–∫—É.")
with col2:
    completeness = int(100 * len(st.session_state.asked) / QUESTIONS_MAX)
    st.progress(min(completeness,100), text=f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–æ—Ñ–∏–ª—è: {completeness}%")

q = next_question()
if q and len(st.session_state.asked) < QUESTIONS_MAX:
    st.subheader(q["text"])
    choice = st.radio("–í–∞—à –æ—Ç–≤–µ—Ç:", LIKERT_LABELS, index=2, horizontal=True, key=f"ans_{q['id']}")
    btns = st.columns(2)
    if btns[0].button("–°–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å"):
        val = LIKERT_VALUES[LIKERT_LABELS.index(choice)]
        if val != 0:  # "–Ω–µ –∑–Ω–∞—é" = 0 ‚Üí –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ–º
            update_theta(val, q["targets"])
        st.session_state.asked.add(q["id"])
        st.rerun()
    show_now = btns[1].button("–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–¥–±–æ—Ä–∫—É —Å–µ–π—á–∞—Å")
else:
    show_now = True

st.divider()

if show_now:
    st.subheader("–ü–æ–¥–±–æ—Ä–∫–∞ –¥–ª—è –≤–∞—Å")
    recs = filter_and_rank(st.session_state.theta, top_k=6)
    for _, row in recs.iterrows():
        with st.container(border=True):
            st.markdown(f"**{row['title']}**  \n–ñ–∞–Ω—Ä—ã: {row['genres']} ¬∑ {int(row['runtime'])} –º–∏–Ω ¬∑ {int(row['year'])}")
            st.caption(explain_card(row, st.session_state.theta))

st.divider()
with st.expander("–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å (Œ∏)"):
    st.json(st.session_state.theta)
